{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Topic models with Gaussian likelihoods\n\nQuick demonstration that you can easily use bnpy to\nperform mixed membership modeling of grouped data with *any* likelihood.\n\nThe basic idea is that we use the same Gaussian mixture model for each\ngroup of data, but the *appearance probabilities* are allowed to be \nlearned in a customized way for each group.\n\nHere, we'll analyze motion capture data from 6 different sequences\nof an individual actor peforming different exercises.\n\nAlthough this data is inherently sequential in nature and it is smart to \nuse a model that accounts for time, we'll ignore that for now and\nfocus on the dataset's *grouped* nature.\n\n\nThat is, we can compare the following two models:\n\n* Baseline: Gaussian mixture model that pools all observations from all sequences\n* Smarter alternative: Latent Dirichlet Allocation with a Gaussian likelihood\n\nThat is, we treat each sequence as a separate collection of data examples,\nmodeled by *group* specific appearance probabilities but *shared* cluster\nmeans and covariances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 1\n\nimport bnpy\nimport numpy as np\nimport os\n\nfrom matplotlib import pylab\nimport seaborn as sns\n\nFIG_SIZE = (5, 5)\nLANDSCAPE_FIG_SIZE = (15, 5)\npylab.rcParams['figure.figsize'] = FIG_SIZE\n\nnp.set_printoptions(precision=3, suppress=1, linewidth=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load dataset from file\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_path = os.path.join(bnpy.DATASET_PATH, 'mocap6')\ndataset = bnpy.data.GroupXData.read_npz(\n    os.path.join(dataset_path, 'dataset.npz'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Function to make a simple plot of the raw data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def show_single_sequence(seq_id):\n    start = dataset.doc_range[seq_id]\n    stop = dataset.doc_range[seq_id + 1]\n    pylab.figure(figsize=LANDSCAPE_FIG_SIZE)\n    for dim in range(12):\n        X_seq = dataset.X[start:stop]\n        pylab.plot(X_seq[:, dim], '.-')\n    pylab.xlabel('time')\n    pylab.ylabel('angle')\n    pylab.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization of the first sequence\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "show_single_sequence(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: hyperparameters\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "K = 10            # Number of clusters/states\n\nalpha = 0.25      # group-level Dirichlet concentration parameter\n\ngamma = 5.0       # top-level Dirichlet concentration parameter (used by HDP only)\n\nsF = 1.0          # Set observation model prior so E[covariance] = identity\nECovMat = 'eye'\n\nnLap = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline: Mixture model with *DiagGauss* observation model\n\nWe'll take the best of 3 independent inits ('tasks')\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mix_model, mix_info_dict = bnpy.run(\n    dataset, 'FiniteMixtureModel', 'DiagGauss', 'memoVB',\n    output_path='/tmp/mocap6/test-model=FiniteMixtureModel+DiagGauss-ECovMat=1*eye/',\n    nLap=nLap, nTask=3, nBatch=1, convergeThr=0.0001,\n    gamma=1.0,\n    sF=sF, ECovMat=ECovMat,\n    K=K, initname='randexamples',\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FiniteTopicModel with *DiagGauss* observation model\n\nWe'll take the best of 3 independent inits ('tasks')\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "finite_model, finite_info_dict = bnpy.run(\n    dataset, 'FiniteTopicModel', 'DiagGauss', 'memoVB',\n    output_path='/tmp/mocap6/test-model=FiniteTopicModel+DiagGauss-ECovMat=1*eye/',\n    nLap=nLap, nTask=3, nBatch=1, convergeThr=0.0001,\n    alpha=alpha,\n    sF=sF, ECovMat=ECovMat,\n    K=K, initname='randexamples',\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HDP-HMM with *DiagGauss* observation model\n\nWe'll take the best of 3 independent inits ('tasks')\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hdp_topic_model, hdp_info_dict = bnpy.run(\n    dataset, 'HDPTopicModel', 'DiagGauss', 'memoVB',\n    output_path='/tmp/mocap6/test-model=HDPTopicModel+DiagGauss-ECovMat=1*eye/',\n    nLap=nLap, nTask=3, nBatch=1, convergeThr=0.0001,\n    gamma=gamma, alpha=alpha,\n    sF=sF, ECovMat=ECovMat,\n    K=K, initname='randexamples',\n    moves='shuffle',\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare loss function traces for all methods\n\nWe'll notice that the simple mixture performs noticeably worse than the \nmore flexible models that allow group-specific cluster weights\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pylab.figure()\npylab.plot(\n    mix_info_dict['lap_history'],\n    mix_info_dict['loss_history'], 'k--',\n    label='mix + diag gauss')\npylab.plot(\n    finite_info_dict['lap_history'],\n    finite_info_dict['loss_history'], 'm.-',\n    label='LDA + diag gauss')\npylab.plot(\n    hdp_info_dict['lap_history'],\n    hdp_info_dict['loss_history'], 'r.-',\n    label='HDP + diag gauss')\npylab.legend(loc='upper right')\npylab.xlabel('num. laps')\npylab.ylabel('loss')\npylab.xlim([0, 200]) # avoid early iterations\npylab.ylim([3.5, 3.7]) # handpicked\npylab.draw()\npylab.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show the baseline per-sequence appearances\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=3, suppress=1, linewidth=200)\n\n## Compute approx. posterior parameter 'resp' for each example\n# resp : 2D array, n_examples x n_clusters\n# resp[n] defines Discrete probability of using clusters to explain each example\nLP = mix_model.calc_local_params(dataset)\nresp_NK = LP['resp']\n\n## Compute the per-sequence average usage.\navg_resp_DK = np.zeros((dataset.nDoc, K))\nfor d in range(dataset.nDoc):\n    start = dataset.doc_range[d]\n    stop = dataset.doc_range[d+1]\n    avg_resp_DK[d] = np.mean(resp_NK[start:stop], axis=0)\n\nprint(\"Baseline mixture model: per-sequence average cluster usage\")\nprint(avg_resp_DK)\n\nnnz = np.sum(avg_resp_DK < 0.001)\nprint(\"Sparsity level: %d/%d entries close-to-zero\" % (\n    nnz, avg_resp_DK.size))\nprint(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show the learned group-specific mixture weights\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compute approx. posterior parameter 'theta' for each document\nLP = finite_model.calc_local_params(dataset)\nresp_NK = LP['resp']\n\navg_resp_DK = np.zeros((dataset.nDoc, K))\nfor d in range(dataset.nDoc):\n    start = dataset.doc_range[d]\n    stop = dataset.doc_range[d+1]\n    avg_resp_DK[d] = np.mean(resp_NK[start:stop], axis=0)\n\nprint(\"LDA mixed membership model: per-sequence average cluster usage\")\nprint(avg_resp_DK)\nnnz = np.sum(avg_resp_DK < 0.001)\nprint(\"Sparsity level: %d/%d entries close-to-zero\" % (\n    nnz, avg_resp_DK.size))\nprint(\"\")\n\n# theta : 2D array, n_docs x n_clusters\n# theta[d] defines Dirichlet parameters for d-th document\ntheta_DK = LP['theta']\n\n# Compute expected probabilities\nE_pi_DK = theta_DK / np.sum(theta_DK, axis=1)[:,np.newaxis]\n\n\nprint(\"LDA mixed membership model: per-sequence cluster probabilities\")\nprint(E_pi_DK)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}