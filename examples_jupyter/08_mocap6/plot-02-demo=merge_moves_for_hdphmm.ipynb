{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Merge moves with HDP-HMM\n\nHow to try merge moves efficiently for time-series datasets.\n\nThis example reviews three possible ways to plan and execute merge\nproposals.\n\n* try merging all pairs of clusters\n* pick fewer merge pairs (at most 5 per cluster) in a size-biased way\n* pick fewer merge pairs (at most 5 per cluster) in objective-driven way\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 2\n\nimport bnpy\nimport numpy as np\nimport os\n\nfrom matplotlib import pylab\nimport seaborn as sns\n\nFIG_SIZE = (10, 5)\npylab.rcParams['figure.figsize'] = FIG_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Load data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Read bnpy's built-in \"Mocap6\" dataset from file.\n\ndataset_path = os.path.join(bnpy.DATASET_PATH, 'mocap6')\ndataset = bnpy.data.GroupXData.read_npz(\n    os.path.join(dataset_path, 'dataset.npz'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Initialization hyperparameters\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "init_kwargs = dict(\n    K=20,\n    initname='randexamples',\n    )\n\nalg_kwargs = dict(\n    nLap=29,\n    nTask=1, nBatch=1, convergeThr=0.0001,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: HDP-HMM hyperparameters\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hdphmm_kwargs = dict(\n    gamma = 5.0,       # top-level Dirichlet concentration parameter\n    transAlpha = 0.5,  # trans-level Dirichlet concentration parameter \n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Gaussian observation model hyperparameters\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gauss_kwargs = dict(\n    sF = 1.0,          # Set prior so E[covariance] = identity\n    ECovMat = 'eye',\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## All-Pairs : Try all possible pairs of merges every 10 laps\n\nThis is expensive, but a good exhaustive test.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "allpairs_merge_kwargs = dict(\n    m_startLap = 10,\n    # Set limits to number of merges attempted each lap.\n    # This value specifies max number of tries for each cluster\n    # Setting this very high (to 50) effectively means try all pairs\n    m_maxNumPairsContainingComp = 50,\n    # Set \"reactivation\" limits\n    # So that each cluster is eligible again after 10 passes thru dataset\n    # Or when it's size changes by 400%\n    m_nLapToReactivate = 10,\n    m_minPercChangeInNumAtomsToReactivate = 400 * 0.01,\n    # Specify how to rank pairs (determines order in which merges are tried)\n    # 'total_size' and 'descending' means try largest combined clusters first\n    m_pair_ranking_procedure = 'total_size',\n    m_pair_ranking_direction = 'descending',\n    )\n\nallpairs_trained_model, allpairs_info_dict = bnpy.run(\n    dataset, 'HDPHMM', 'DiagGauss', 'memoVB',\n    output_path='/tmp/mocap6/trymerge-K=20-model=HDPHMM+DiagGauss-ECovMat=1*eye-merge_strategy=all_pairs/',\n    moves='merge,shuffle',\n    **dict(\n        sum(map(list,   [alg_kwargs.items(),\n                        init_kwargs.items(),\n                        hdphmm_kwargs.items(),\n                        gauss_kwargs.items(),\n                        allpairs_merge_kwargs.items()]),[]))\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Large-Pairs : Try 5-largest-size pairs of merges every 10 laps\n\nThis is much cheaper than all pairs. Let's see how well it does.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "largepairs_merge_kwargs = dict(\n    m_startLap = 10,\n    # Set limits to number of merges attempted each lap.\n    # This value specifies max number of tries for each cluster\n    m_maxNumPairsContainingComp = 5,\n    # Set \"reactivation\" limits\n    # So that each cluster is eligible again after 10 passes thru dataset\n    # Or when it's size changes by 400%\n    m_nLapToReactivate = 10,\n    m_minPercChangeInNumAtomsToReactivate = 400 * 0.01,\n    # Specify how to rank pairs (determines order in which merges are tried)\n    # 'total_size' and 'descending' means try largest size clusters first\n    m_pair_ranking_procedure = 'total_size',\n    m_pair_ranking_direction = 'descending',\n    )\n\n\nlargepairs_trained_model, largepairs_info_dict = bnpy.run(\n    dataset, 'HDPHMM', 'DiagGauss', 'memoVB',\n    output_path='/tmp/mocap6/trymerge-K=20-model=HDPHMM+DiagGauss-ECovMat=1*eye-merge_strategy=large_pairs/',\n    moves='merge,shuffle',\n    **dict(\n        sum(map(list,   [alg_kwargs.items(),\n                        init_kwargs.items(),\n                        hdphmm_kwargs.items(),\n                        gauss_kwargs.items(),\n                        largepairs_merge_kwargs.items()]),[])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Good-ELBO-Pairs : Rank pairs of merges by improvement to observation model\n\nThis is much cheaper than all pairs and perhaps more principled.\nLet's see how well it does.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "goodelbopairs_merge_kwargs = dict(\n    m_startLap = 10,\n    # Set limits to number of merges attempted each lap.\n    # This value specifies max number of tries for each cluster\n    m_maxNumPairsContainingComp = 5,\n    # Set \"reactivation\" limits\n    # So that each cluster is eligible again after 10 passes thru dataset\n    # Or when it's size changes by 400%\n    m_nLapToReactivate = 10,\n    m_minPercChangeInNumAtomsToReactivate = 400 * 0.01,\n    # Specify how to rank pairs (determines order in which merges are tried)\n    # 'obsmodel_elbo' means rank pairs by improvement to observation model ELBO\n    m_pair_ranking_procedure = 'obsmodel_elbo',\n    m_pair_ranking_direction = 'descending',\n    )\n\n\ngoodelbopairs_trained_model, goodelbopairs_info_dict = bnpy.run(\n    dataset, 'HDPHMM', 'DiagGauss', 'memoVB',\n    output_path='/tmp/mocap6/trymerge-K=20-model=HDPHMM+DiagGauss-ECovMat=1*eye-merge_strategy=good_elbo_pairs/',\n    moves='merge,shuffle',\n    **dict(\n        sum(map(list,   [alg_kwargs.items(),\n                        init_kwargs.items(),\n                        hdphmm_kwargs.items(),\n                        gauss_kwargs.items(),\n                        goodelbopairs_merge_kwargs.items()]),[])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare loss function vs wallclock time\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pylab.figure()\nfor info_dict, color_str, label_str in [\n        (allpairs_info_dict, 'k', 'all_pairs'),\n        (largepairs_info_dict, 'g', 'large_pairs'),\n        (goodelbopairs_info_dict, 'b', 'good_elbo_pairs')]:\n    pylab.plot(\n        info_dict['elapsed_time_sec_history'],\n        info_dict['loss_history'],\n        '.-',\n        color=color_str,\n        label=label_str)\npylab.legend(loc='upper right')\npylab.xlabel('elapsed time (sec)')\npylab.ylabel('loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare number of active clusters vs wallclock time\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pylab.figure()\nfor info_dict, color_str, label_str in [\n        (allpairs_info_dict, 'k', 'all_pairs'),\n        (largepairs_info_dict, 'g', 'large_pairs'),\n        (goodelbopairs_info_dict, 'b', 'good_elbo_pairs')]:\n    pylab.plot(\n        info_dict['elapsed_time_sec_history'],\n        info_dict['K_history'],\n        '.-',\n        color=color_str,\n        label=label_str)\npylab.legend(loc='upper right')\npylab.xlabel('elapsed time (sec)')\npylab.ylabel('num. clusters (K)')\n\npylab.show(block=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}