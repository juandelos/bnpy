{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Visualizing learned state sequences and transition probabilities\n\nTrain a sticky HMM model with auto-regressive Gaussian likelihoods on\nsmall motion capture data. Discover how the likelihood hyperparameters\nmight impact performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 3\n\nimport bnpy\nimport numpy as np\nimport os\n\nimport matplotlib\nfrom matplotlib import pylab\nimport seaborn as sns\n\nnp.set_printoptions(suppress=1, precision=3)\n\nFIG_SIZE = (10, 5)\npylab.rcParams['figure.figsize'] = FIG_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load dataset from file\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_path = os.path.join(bnpy.DATASET_PATH, 'mocap6', 'dataset.npz')\n\ndataset = bnpy.data.GroupXData.read_npz(dataset_path)\ndataset_biasfeature = bnpy.data.GroupXData(\n    X=dataset.X,\n    Xprev=np.hstack([dataset.Xprev, np.ones((dataset.X.shape[0], 1))]),\n    doc_range=dataset.doc_range)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Function to make a simple plot of the raw data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def show_single_sequence(\n        seq_id,\n        zhat_T=None,\n        z_img_cmap=None,\n        ylim=[-120, 120],\n        K=5,\n        left=0.2, bottom=0.2, right=0.8, top=0.95):\n    if z_img_cmap is None:\n        z_img_cmap = matplotlib.cm.get_cmap('Set1', K)\n\n    if zhat_T is None:\n        nrows = 1\n    else:\n        nrows = 2\n    fig_h, ax_handles = pylab.subplots(\n        nrows=nrows, ncols=1, sharex=True, sharey=False)\n    ax_handles = np.atleast_1d(ax_handles).flatten().tolist()\n\n    start = dataset.doc_range[seq_id]\n    stop = dataset.doc_range[seq_id + 1]\n    # Extract current sequence\n    # as a 2D array : T x D (n_timesteps x n_dims)\n    curX_TD = dataset.X[start:stop]\n    for dim in range(12):\n        ax_handles[0].plot(curX_TD[:, dim], '.-')\n    ax_handles[0].set_ylabel('angle')\n    ax_handles[0].set_ylim(ylim)\n    z_img_height = int(np.ceil(ylim[1] - ylim[0]))\n    pylab.subplots_adjust(\n        wspace=0.1,\n        hspace=0.1,\n        left=left, right=right,\n        bottom=bottom, top=top)\n    if zhat_T is not None:\n        img_TD = np.tile(zhat_T, (z_img_height, 1))\n        ax_handles[1].imshow(\n            img_TD,\n            interpolation='nearest',\n            vmin=-0.5, vmax=(K-1)+0.5,\n            cmap=z_img_cmap)\n        ax_handles[1].set_ylim(0, z_img_height)\n        ax_handles[1].set_yticks([])\n\n        bbox = ax_handles[1].get_position()\n        width = (1.0 - bbox.x1) / 3\n        height = bbox.y1 - bbox.y0\n        cax = fig_h.add_axes([right + 0.01, bottom, width, height])\n        cbax_h = fig_h.colorbar(\n            ax_handles[1].images[0], cax=cax, orientation='vertical')\n        cbax_h.set_ticks(np.arange(K))\n        cbax_h.set_ticklabels(np.arange(K))\n        cbax_h.ax.tick_params(labelsize=9)\n\n    ax_handles[-1].set_xlabel('time')\n    return ax_handles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization of the first sequence (1 of 6)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "show_single_sequence(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup training: hyperparameters\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "K = 5            # Number of clusters/states\n\n# Allocation model (Standard Bayesian HMM with sticky self-transitions)\ntransAlpha =   0.5  # trans-level Dirichlet concentration parameter \nstartAlpha =  10.0  # starting-state Dirichlet concentration parameter\nhmmKappa   =  50.0  # set sticky self-transition weight\n\n# Observation model (1st-order Auto-regressive Gaussian)\nsF = 1.0          # Set observation model prior so E[covariance] = identity\nECovMat = 'eye'\n\nnTask = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train with EM an HMM with *AutoRegGauss* observation model\n\nTrain single model using likelihood without any free parameter\n\n$\n      x_t ~ \\mbox{Normal}( A_k x_t-1, \\Sigma_k)\n$\n\n\nTake the best of 10 random initializations (in terms of evidence lower bound).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bias0_trained_model, bias0_info_dict = bnpy.run(\n    dataset,\n    'FiniteHMM', 'AutoRegGauss', 'EM',\n    output_path=(\n        '/tmp/mocap6/bias0-K=%d-model=HMM+AutoRegGauss-ECovMat=1*eye/'\n        % (K)),\n    nLap=100, nTask=nTask, convergeThr=0.0001,\n    transAlpha=transAlpha, startAlpha=startAlpha, hmmKappa=hmmKappa,\n    sF=sF, ECovMat=ECovMat, MMat='eye',\n    K=K, initname='randexamples',\n    printEvery=25,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train with EM an HMM with *AutoRegGauss* observation model\n\nTrain single model using likelihood WITH free mean parameter\n\n$\n      x_t ~ \\mbox{Normal}( A_k x_t-1 + \\mu_k, \\Sigma_k)\n$\n\nThis is equivalent to including column of all ones into the x-previous,\nwhich is how we do it in practice...\n\nTake the best of 10 random initializations (in terms of evidence lower bound).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bias1_trained_model, bias1_info_dict = bnpy.run(\n    dataset_biasfeature,\n    'FiniteHMM', 'AutoRegGauss', 'EM',\n    output_path=(\n        '/tmp/mocap6/bias1-K=%d-model=HMM+AutoRegGauss-ECovMat=1*eye/'\n        % (K)),\n    nLap=100, nTask=nTask, convergeThr=0.0001,\n    transAlpha=transAlpha, startAlpha=startAlpha, hmmKappa=hmmKappa,\n    sF=sF, ECovMat=ECovMat, MMat='eye',\n    K=K, initname='randexamples',\n    printEvery=25,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train with VB an HMM with *AutoRegGauss* observation model\n\nTrain single model using likelihood with free mean parameter\n\n$\n      x_t ~ \\mbox{Normal}( A_k x_t-1 + \\mu_k, \\Sigma_k)\n$\n\nThis is equivalent to including column of all ones into the x-previous.\n\nTake the best of 10 random initializations (in terms of evidence lower bound).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bias1vb_trained_model, bias1vb_info_dict = bnpy.run(\n    dataset_biasfeature,\n    'FiniteHMM', 'AutoRegGauss', 'VB',\n    output_path=(\n        '/tmp/mocap6/bias1vb-K=%d-model=HMM+AutoRegGauss-ECovMat=1*eye/'\n        % (K)),\n    nLap=100, nTask=nTask, convergeThr=0.0001,\n    transAlpha=transAlpha, startAlpha=startAlpha, hmmKappa=hmmKappa,\n    sF=sF, ECovMat=ECovMat, MMat='eye',\n    K=K, initname=bias1_info_dict['task_output_path'],\n    printEvery=25,\n    )\n\n\nbias0vb_trained_model, bias0vb_info_dict = bnpy.run(\n    dataset,\n    'FiniteHMM', 'AutoRegGauss', 'VB',\n    output_path=(\n        '/tmp/mocap6/bias1vb-K=%d-model=HMM+AutoRegGauss-ECovMat=1*eye/'\n        % (K)),\n    nLap=100, nTask=nTask, convergeThr=0.0001,\n    transAlpha=transAlpha, startAlpha=startAlpha, hmmKappa=hmmKappa,\n    sF=sF, ECovMat=ECovMat, MMat='eye',\n    K=K, initname=bias0_info_dict['task_output_path'],\n    printEvery=25,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare loss function traces for all methods\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pylab.figure()\nmarkersize = 5\npylab.plot(\n    bias0_info_dict['lap_history'],\n    bias0_info_dict['loss_history'], 'bs-',\n    markersize=markersize,\n    label='EM without bias feature')\npylab.plot(\n    bias1_info_dict['lap_history'],\n    bias1_info_dict['loss_history'], 'ks-',\n    markersize=markersize,\n    label='EM WITH bias feature')\npylab.plot(\n    bias0vb_info_dict['lap_history'],\n    bias0vb_info_dict['loss_history'], 'bd--',\n    markersize=markersize,\n    label='VB without bias feature')\npylab.plot(\n    bias1vb_info_dict['lap_history'],\n    bias1vb_info_dict['loss_history'], 'kd--',\n    markersize=markersize,\n    label='VB WITH bias feature')\n\npylab.legend(loc='upper right')\npylab.ylim([1.5, 3.0])\n\npylab.xlabel('num. laps')\npylab.ylabel('loss: - log p(x)')\npylab.draw()\npylab.tight_layout()\n\n\npylab.show(block=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}