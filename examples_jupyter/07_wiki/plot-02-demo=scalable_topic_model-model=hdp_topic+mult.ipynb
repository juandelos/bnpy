{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Scalable training of HDP topic models\n\nIn this demo, we'll review the scalable memoized training of HDP topic models.\n\nTo review, our memoized VB algorithm (Hughes and Sudderth, NeurIPS 2013) proceeds like this pseudocode:\n\n```python\nn_laps_completed = 0\nwhile n_laps_completed < nLap:\n\n    n_batches_completed_this_lap = 0\n    while n_batches_completed_this_lap < nBatch:\n\n        batch_data = next_minibatch()\n\n        # Batch-specific local step\n        LPbatch = model.calc_local_params(batch_data, **local_step_kwargs)\n\n        # Batch-specific summary step\n        SSbatch = model.get_summary_stats(batch_data, LPbatch)\n\n        # Increment global summary statistics\n        SS = update_global_stats(SS, SSbatch)\n\n        # Update global parameters\n        model.update_global_params(SS)\n```\nFrom a runtime perspective, the important settings a user can control are:\n\n* nBatch: the number of batches\n* nLap  : the number of required laps (passes thru full dataset) to perform\n* local_step_kwargs : dict of keyword arguments that control local step optimization\n\n## What happens at each step?\nIn the local step, we visit each document in the current batch.\nAt each document, we estimate its local (document-specific) variational posterior.\nThis is done via an *iterative* algorithm, which is rather expensive.\nWe might need 50 or 100 or 200 iterations at each document, though each iteration is linear in the number of documents and the number of topics.\n\nThe summary step simply computes the sufficient statistics for the batch.\nUsually this is far faster than the local step, since it a closed-form computation not an iterative estimation.\n\nThe global parameter update step is similarly quite fast, because we're using a model that enjoys conjugacy (e.g. the observation model's global posterior is a Dirichlet, related to a Multinomial likelihood and a Dirichlet prior). \n\nThus, the *local step* is the runtime bottleneck.\n\n\n## Runtime vs nBatch\nIt may be tempting to think that smaller minibatches (increasing nBatch) will make the code go \"faster\".\nHowever, if you fix the number of laps to be completed, increasing the number of batches leads to strictly *more* work.\n\nHowever, for each of the requested laps, here's the work performed:\n\n* the *same* number of per-document local update iterations are completed\n* the *same* number of per-document summaries are completed\n* the total number of global parameter updates is exactly nBatch\n\nFor scaling to large datasets, the important thing is *not* to keep the number of laps the same, but to keep the wallclock runtime the same, and then to ask how much progress is made in reducing the loss (either training loss or validation loss, whichever is more relevant). Running with larger nBatch values will usually give improved progress in the same amount of time.\n\n\n## Runtime vs Local Step Convergence Thresholds\nSince the local step dominates the cost of updates, managing the run time of the local iterations is important.\n\nThere are two settings in the code that control this:\n\n* nCoordAscentItersLP : number of local step iterations to perform per document\n* convThrLP : threshold to decide if local step updates have converged\n\nThe local step pseudocode is:\n\n```python\nfor each document d:\n\n    for iter in [1, 2, ..., nCoordAscentItersLP]:\n\n        # Update q(\\pi_d), the variational posterior for document d's\n        # topic probability vector\n\n        # Update q(z_d), the variational posterior for document d's\n        # topic-word discrete assignments\n\n        # Compute N_d1, ... N_dK, expected count of topic k in document d\n\n        if iter % 5 == 0: # every 5 iterations, check for early convergence\n\n            # Quit early if no N_dk entry changes by more than convThrLP\n```\n```\n\nThus, setting these local step optimization hyperparameters can be very practically important.\n\nSetting convThrLP to -1 (or any number less than zero) will always do all the requested iterations.\nSetting convThrLP to something moderate (like 0.05) will often reduce the local step cost by 2x or more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import bnpy\nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read text dataset from file\n\nKeep the first 6400 documents so we have a nice even number\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_path = os.path.join(bnpy.DATASET_PATH, 'wiki')\ndataset = bnpy.data.BagOfWordsData.LoadFromFile_ldac(\n    os.path.join(dataset_path, 'train.ldac'),\n    vocabfile=os.path.join(dataset_path, 'vocab.txt'))\n \n# Keep 6400 documents with at least 50 words\ndoc_ids = np.flatnonzero(dataset.getDocTypeCountMatrix().sum(axis=1) >= 50)\ndataset = dataset.make_subset(docMask=doc_ids[:6400], doTrackFullSize=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train scalable HDP topic models\n\nVary the number of batches and the local step convergence threshold\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Model kwargs\ngamma = 25.0\nalpha = 0.5\nlam = 0.1\n\n# Initialization kwargs\nK = 25 \n\n# Algorithm kwargs\nnLap = 5\ntraceEvery = 0.5\nprintEvery = 0.5\nconvThr = 0.01\n\nfor row_id, convThrLP in enumerate([-1.00, 0.25]):\n\n    local_step_kwargs = dict(\n        # perform at most this many iterations at each document\n        nCoordAscentItersLP=100,\n        # stop local iters early when max change in doc-topic counts < this thr\n        convThrLP=convThrLP,\n        )\n\n    for nBatch in [1, 16]:\n        \n        output_path = '/tmp/wiki/scalability-model=hdp_topic+mult-alg=memoized-nBatch=%d-nCoordAscentItersLP=%s-convThrLP=%.3g/' % (\n                nBatch, local_step_kwargs['nCoordAscentItersLP'], convThrLP)\n\n        trained_model, info_dict = bnpy.run(\n            dataset, 'HDPTopicModel', 'Mult', 'memoVB',\n            output_path=output_path,\n            nLap=nLap, nBatch=nBatch, convThr=convThr,\n            K=K, gamma=gamma, alpha=alpha, lam=lam,\n            initname='randomlikewang', \n            moves='shuffle',\n            traceEvery=traceEvery, printEvery=printEvery,\n            **local_step_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot: Training Loss and Laps Completed vs. Wallclock time\n\n* Left column: Training Loss progress vs. wallclock time\n* Right column: Laps completed vs. wallclock time\n\nRemember: one lap is a complete pass through entire training set (6400 docs)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "H = 3; W = 4\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(2*W,2*H), sharex=True, sharey=False)    \n \nfor row_id, convThrLP in enumerate([-1.00, 0.25]):\n        \n    for nBatch in [1, 16]:\n\n        output_path = '/tmp/wiki/scalability-model=hdp_topic+mult-alg=memoized-nBatch=%d-nCoordAscentItersLP=%s-convThrLP=%.3g/' % (\n            nBatch, local_step_kwargs['nCoordAscentItersLP'], convThrLP)\n\n        elapsed_time_T = np.loadtxt(os.path.join(output_path, '1', 'trace_elapsed_time_sec.txt'))\n        elapsed_laps_T = np.loadtxt(os.path.join(output_path, '1', 'trace_lap.txt'))\n        loss_T = np.loadtxt(os.path.join(output_path, '1', 'trace_loss.txt'))\n    \n        ax[row_id, 0].plot(elapsed_time_T, loss_T, '.-', label='nBatch=%d, batch_size = %d' % (nBatch, 6400/nBatch))\n        ax[row_id, 1].plot(elapsed_time_T, elapsed_laps_T, '.-', label='nBatch=%d' % nBatch)\n\n        ax[row_id, 0].set_ylabel('training loss')\n        ax[row_id, 1].set_ylabel('laps completed')\n\n        ax[row_id, 0].set_xlabel('elapsed time (sec)')\n        ax[row_id, 1].set_xlabel('elapsed time (sec)')\n    ax[row_id, 0].legend(loc='upper right')\n    ax[row_id, 0].set_title(('Loss vs Time, local conv. thr. %.2f' % (convThrLP)).replace(\".00\", \"\"))\n    ax[row_id, 1].set_title(('Laps vs Time, local conv. thr. %.2f' % (convThrLP)).replace(\".00\", \"\"))\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lessons Learned\n\nThe local step is the most expensive step in terms of runtime (far more costly than the summary or global step)\nGenerally, increasing the number of batches has the following effect:\n* Increase the total computational work that must be done for a fixed number of laps\n* Improve the model quality achieved in a limited amount of time, unless the batch size becomes so small that global parameter estimates are poor\n\nWe generally recommend considering:\n* batch size around 250 - 2000 (which means set nBatch = nDocsTotal / batch_size)\n* carefully setting the local step convergence threshold (convThrLP could be 0.05 or 0.25 when training, probably needs to be smaller when computing likelihoods for a document)\n* setting the number of iterations per document sufficiently large (might get away with nCoordAscentItersLP = 10 or 25 when training, but might need many iters like 50 or 100 at least when evaluating likelihoods to be confident in the value)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}