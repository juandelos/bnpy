{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Demonstration of Sparse Responsibilities for Mixtures of Gaussians\n\nThis demo illustrates \"sparse responsibilities\" for mixture models.\n\nIn this example, we show how BNPy makes it easy to vary\nper-example inference, so that for a 3-cluster model\nwe can manually try each of the following:\n\n* each data point assigned to up to 1 cluster\n* each data point assigned to up to 2 clusters\n* each data point assigned to up to 3 clusters\n\nIn our variational approximation, when we fit\na K-cluster mixture model to a specific data point,\nwe learn a vector of posterior **responsibilities** --\nwe often call these \"resp\" for short in the code --\nindicating how much of the unit probability mass for this example\nis explained by each of the K clusters.\n\nFor example, for a specific data point we might have a resp vector of:\n\n    [ 0.      0.9262  0.0738]\n\nwhich we interpret as saying this point is 92% explained by the \n2nd cluster, 7% by the 3rd cluster, and 0% by the first.\n\nThis short demo just shows how we can deliberately enforce sparsity\nin learned responsibilities, so that each example may have only L \nnon-zero entries in its resp vector. For example, we'll fit a K=3 model\nbelow, and show how we can enforce L=1 or L=2 sparsity. \n\nFor a technical introduction, see our ArXiv paper:\n\n    Michael C. Hughes and Erik B. Sudderth (2016)\n    \"Fast Learning of Clusters and Topics via Sparse Posteriors.\"\n    https://arxiv.org/abs/1609.07521\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n\nimport bnpy\nimport numpy as np\nimport os\n\nfrom matplotlib import pylab\nimport seaborn as sns\n\nSMALL_FIG_SIZE = (2.5, 2.5)\nFIG_SIZE = (5, 5)\npylab.rcParams['figure.figsize'] = FIG_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load dataset from file\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_path = os.path.join(bnpy.DATASET_PATH, 'faithful')\ndataset = bnpy.data.XData.read_csv(\n    os.path.join(dataset_path, 'faithful.csv'))\n\n# Identify \"target\" example ids to focus on\ntarget_example_ids = np.asarray(\n    [ 83,  46, 152, 173,  23, 164, 243, 239, 154, 100])\ntargeted_dataset = bnpy.data.XData(\n    dataset.X[target_example_ids])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make a simple plot of the raw data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pylab.plot(dataset.X[:, 0], dataset.X[:, 1], 'k.')\npylab.xlabel(dataset.column_names[0])\npylab.ylabel(dataset.column_names[1])\npylab.tight_layout()\npylab.title('Complete Dataset')\ndata_ax_h = pylab.gca()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Func to print resp array nicely\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def pprint_sparse_or_dense_resp(LP, target_example_ids=target_example_ids):\n    np.set_printoptions(suppress=True, precision=4)\n    if 'spR' in LP:\n        # sparse case\n        resp_NK = LP['spR'][target_example_ids].toarray()\n    else:\n        resp_NK = LP['resp'][target_example_ids]\n    print(resp_NK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VB with DP mixture model and diagonal Gaussian observations\n\nAssumes exactly 3 clusters\n\nAssumes diagonal covariances.\n\nNo sparsity assumptions during training\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "K = 3          # n clusters\ngamma = 50.0   # DP concentration param\nsF = 0.1       # scale of expected covariance\n\nfull_trained_model, full_info_dict = bnpy.run(\n    dataset, 'DPMixtureModel', 'DiagGauss', 'VB',\n    output_path='/tmp/faithful/demo_sparse_resp-K=3-lik=Gauss-ECovMat=5*eye/',\n    nLap=1000, nTask=5, nBatch=1, convergeThr=0.0001,\n    gamma0=gamma, sF=sF, ECovMat='eye',\n    K=K, initname='randexamples',\n    )\n\n# Add this model into the current plot\nbnpy.viz.PlotComps.plotCompsFromHModel(\n    full_trained_model,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Do inference with L=1 sparsity\n\nKwarg to set is \"nnzPerRowLP = 1\"\n\nnnzPerRowLP is read as \"number of non-zero entries per row of local parameters\"\n\nThis enforces that each row of resp array has ___ non zero entries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('SPARSITY LEVEL: 1 of 3')\nLP1 = full_trained_model.calc_local_params(dataset, nnzPerRowLP=1)\npprint_sparse_or_dense_resp(LP1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Do inference with L=2 sparsity\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('SPARSITY LEVEL: 2 of 3')\nLP2 = full_trained_model.calc_local_params(dataset, nnzPerRowLP=2)\npprint_sparse_or_dense_resp(LP2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Do inference with L=3 sparsity\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('SPARSITY LEVEL: 3 of 3 (DENSE)')\nLP3 = full_trained_model.calc_local_params(dataset, nnzPerRowLP=3)\npprint_sparse_or_dense_resp(LP3)\n\n\n# Show the model\npylab.figure(2)\nbnpy.viz.PlotComps.plotCompsFromHModel(\n    full_trained_model,\n    dataset=targeted_dataset,\n    )\npylab.xlabel(dataset.column_names[0])\npylab.ylabel(dataset.column_names[1])\npylab.tight_layout()\npylab.title('Trained model with targeted examples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to incorporate sparsity during training?\nJust pass \"nnzPerRowLP\" kwarg to bnpy.run(...)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}