{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Variational training for Mixtures of Gaussians\n\nShowcase of different models and algorithms applied to same dataset.\n\nIn this example, we show how bnpy makes it easy to apply\ndifferent models and algorithms to the same dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import bnpy\nimport numpy as np\nimport os\n\nfrom matplotlib import pylab\nimport seaborn as sns\n\nSMALL_FIG_SIZE = (2.5, 2.5)\nFIG_SIZE = (5, 5)\npylab.rcParams['figure.figsize'] = FIG_SIZE\n\nnp.set_printoptions(precision=3, suppress=1, linewidth=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load dataset from file\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_path = os.path.join(bnpy.DATASET_PATH, 'faithful')\ndataset = bnpy.data.XData.read_csv(\n    os.path.join(dataset_path, 'faithful.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make a simple plot of the raw data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pylab.plot(dataset.X[:, 0], dataset.X[:, 1], 'k.')\npylab.xlabel(dataset.column_names[0])\npylab.ylabel(dataset.column_names[1])\npylab.tight_layout()\ndata_ax_h = pylab.gca()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Helper function to display the learned clusters\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def show_clusters_over_time(\n        task_output_path=None,\n        query_laps=[0, 1, 2, 10, 20, None],\n        nrows=2):\n    ''' Show 2D elliptical contours overlaid on raw data.\n    '''\n    ncols = int(np.ceil(len(query_laps) // float(nrows)))\n    fig_handle, ax_handle_list = pylab.subplots(\n        figsize=(SMALL_FIG_SIZE[0] * ncols, SMALL_FIG_SIZE[1] * nrows),\n        nrows=nrows, ncols=ncols, sharex=True, sharey=True)\n    for plot_id, lap_val in enumerate(query_laps):\n        cur_model, lap_val = bnpy.load_model_at_lap(task_output_path, lap_val)\n        cur_ax_handle = ax_handle_list.flatten()[plot_id]\n        bnpy.viz.PlotComps.plotCompsFromHModel(\n            cur_model, dataset=dataset, ax_handle=cur_ax_handle)\n        cur_ax_handle.set_title(\"lap: %d\" % lap_val)\n        cur_ax_handle.set_xlabel(dataset.column_names[0])\n        cur_ax_handle.set_ylabel(dataset.column_names[1])\n        cur_ax_handle.set_xlim(data_ax_h.get_xlim())\n        cur_ax_handle.set_ylim(data_ax_h.get_ylim())\n    pylab.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *DiagGauss* observation model\n\nAssume diagonal covariances.\n\nStart with too many clusters (K=20)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gamma = 5.0\nsF = 5.0\nK = 20\n\ndiag_trained_model, diag_info_dict = bnpy.run(\n    dataset, 'DPMixtureModel', 'DiagGauss', 'memoVB',\n    output_path='/tmp/faithful/showcase-K=20-lik=DiagGauss-ECovMat=5*eye/',\n    nLap=1000, nTask=1, nBatch=1, convergeThr=0.0001,\n    gamma0=gamma, sF=sF, ECovMat='eye',\n    K=K, initname='randexamples',\n    )\nshow_clusters_over_time(diag_info_dict['task_output_path'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Gauss* observations + VB\n\nAssume full covariances.\n\nStart with too many clusters (K=20)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "full_trained_model, full_info_dict = bnpy.run(\n    dataset, 'DPMixtureModel', 'Gauss', 'VB',\n    output_path='/tmp/faithful/showcase-K=20-lik=Gauss-ECovMat=5*eye/',\n    nLap=1000, nTask=1, nBatch=1, convergeThr=0.0001,\n    gamma0=gamma, sF=sF, ECovMat='eye',\n    K=K, initname='randexamples',\n    )\nshow_clusters_over_time(full_info_dict['task_output_path'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *ZeroMeanGauss* observations + VB\n\nAssume full covariances and fix all means to zero.\n\nStart with too many clusters (K=20)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "zm_trained_model, zm_info_dict = bnpy.run(\n    dataset, 'DPMixtureModel', 'ZeroMeanGauss', 'VB',\n    output_path='/tmp/faithful/showcase-K=20-lik=ZeroMeanGauss-ECovMat=5*eye/',\n    nLap=1000, nTask=1, nBatch=1, convergeThr=0.0001,\n    gamma0=gamma, sF=sF, ECovMat='eye',\n    K=K, initname='randexamples',\n    )\nshow_clusters_over_time(zm_info_dict['task_output_path'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Gauss* observations + stochastic VB\n\nAssume full covariances and fix all means to zero.\n\nStart with too many clusters (K=20)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stoch_trained_model, stoch_info_dict = bnpy.run(\n    dataset, 'DPMixtureModel', 'Gauss', 'soVB',\n    output_path=\\\n        '/tmp/faithful/showcase-K=20-lik=Gauss-ECovMat=5*eye-alg=soVB/',\n    nLap=50, nTask=1, nBatch=50,\n    rhoexp=0.51, rhodelay=1.0,\n    gamma0=gamma, sF=sF, ECovMat='eye',\n    K=K, initname='randexamples',\n    )\nshow_clusters_over_time(stoch_info_dict['task_output_path'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare loss function traces for all methods\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pylab.figure()\n\npylab.plot(\n    zm_info_dict['lap_history'],\n    zm_info_dict['loss_history'], 'b.-',\n    label='full_covar zero_mean')\npylab.plot(\n    full_info_dict['lap_history'],\n    full_info_dict['loss_history'], 'k.-',\n    label='full_covar')\npylab.plot(\n    diag_info_dict['lap_history'],\n    diag_info_dict['loss_history'], 'r.-',\n    label='diag_covar')\npylab.plot(\n    stoch_info_dict['lap_history'],\n    stoch_info_dict['loss_history'], 'c.:',\n    label='full_covar stochastic')\npylab.legend(loc='upper right')\npylab.xlabel('num. laps')\npylab.ylabel('loss')\npylab.xlim([4, 100]) # avoid early iterations\npylab.ylim([2.34, 4.0]) # handpicked\npylab.draw()\npylab.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect the learned distribution over appearance probabilities\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# E_proba_K : 1D array, size n_clusters\n# Each entry gives expected probability of that cluster\n\nE_proba_K = stoch_trained_model.allocModel.get_active_comp_probs()\n\nprint(\"probability of each cluster:\")\nprint(E_proba_K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect the learned means and covariance distributions\n\nRemember that each cluster has the following approximate posterior\nover its mean vector $\\mu$ and covariance matrix $\\Sigma$:\n\n$$\nq(\\mu, \\Sigma) = \\Normal(\\mu | m, \\kappa \\Sigma) \\Wishart(\\Sigma | \\nu, S)\n$$\n\nWe show here how to compute the expected mean of $\\mu$ and $\\Sigma$\nfrom a trained model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for k in range(K):\n    E_mu_k = stoch_trained_model.obsModel.get_mean_for_comp(k)\n    E_Sigma_k = stoch_trained_model.obsModel.get_covar_mat_for_comp(k)\n    print(\"\")\n    print(\"mean[k=%d]\" % k)\n    print(E_mu_k)\n    print(\"covar[k=%d]\" % k)\n    print(E_Sigma_k)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}